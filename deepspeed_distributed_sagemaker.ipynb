{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37fbfc3d-60e3-4c45-a205-11a7859c1bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps --quiet s3fs==2023.12.2\n",
    "!pip install --quiet transformers==4.41.1 peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5f0097-f6a2-4395-9e86-98eff8c7da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::859967598519:role/service-role/AmazonSageMaker-ExecutionRole-20240523T230081\n",
      "sagemaker bucket: sagemaker-us-east-1-859967598519\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "## Init the aws client\n",
    "###########################\n",
    "import os\n",
    "import sys\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa9ad04-47a4-4dbb-9484-3e205a415210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dolly size: 1500\n",
      "{'instruction': 'When did Virgin Australia start operating?', 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "## Load Dolly dataset\n",
    "###########################\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "dataset = load_dataset('databricks/databricks-dolly-15k', split='train')\n",
    "dataset = dataset.select(range(1500))\n",
    "print(f'Dataset Dolly size: {len(dataset)}')\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0701f792-535f-4177-a844-2762a9a928a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Function to format the dataset to Mistral format\n",
    "###########################\n",
    "\n",
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3df4e049-01d7-4a18-9b6f-6de7f29b2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import config\n",
    " \n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "my_huggingface_token = config.my_huggingface_token;\n",
    "if my_huggingface_token == 'YOUR_HUGGING_FACE_TOKEN':\n",
    "    sys.exit('You need to provide HuggingFace token in config.py file')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=my_huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3550472f-c72f-419d-98a5-0bc6df45632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking dataset into chunks of 2048 tokens.\n",
      "Total number of samples: 149\n",
      "Total number of samples after packing: 149\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "## Format the dataset to Mistral format\n",
    "###########################\n",
    "\n",
    "# add utils method to path for loading dataset\n",
    "sys.path.append(\"scripts\")\n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=2048) # We use 2048 as the maximum length for packing\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples after packing: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08f9bd3-d6d1-4da1-b3dd-1646f223d81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ce7949669342dfab9a51effb92aca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-859967598519/processed/mistral/dolly/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/mistral/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bb1339f-f679-46ce-b4bf-5908dc06aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed_parameters = {\n",
    "  \"deepspeed\": \"./configs/mistral_z3_config_bf16.json\", # deepspeed config file\n",
    "  \"training_script\": \"./scripts/run_qlora.py\" # real training script, not entrypoint\n",
    "}\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "training_hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 3,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 6,                 # batch size for training  REQUIRED REVIEW\n",
    "  'per_device_eval_batch_size': 8,                  # REQUIRED REVIEW\n",
    "  #'gradient_accumulation_steps': 8,                 # Number of updates steps to accumulate\n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'bf16': True,                                     # use bfloat16 precision\n",
    "  'tf32': True,                                     # use tf32 precision\n",
    "  'learning_rate': 2e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"constant\",                   # learning rate scheduler\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': False,                          # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'save_strategy': \"epoch\",                         # save strategy for checkpoints\n",
    "  'save_total_limit': 3,\n",
    "  'output_dir': '/opt/ml/checkpoints',              # output directory, where to save assets during training\n",
    "                                                    # could be used for checkpointing. The final trained\n",
    "                                                    # model will always be saved to s3 at the end of training\n",
    "  'hf_token': my_huggingface_token,\n",
    "}\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "job_name = f'mistral-deepspeed-qlora-{training_hyperparameters[\"model_id\"].replace(\"/\",\"-\").replace(\".\",\"-\")}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'ds_launcher.py',    # train script\n",
    "    source_dir           = '.',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 2,                 # the number of instances used for training\n",
    "    max_run              = 3600,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',           # the python version used in the training job\n",
    "    hyperparameters      =  {\n",
    "        **training_hyperparameters,  # the hyperparameters passed to the training job\n",
    "        **deepspeed_parameters,\n",
    "    },\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "\n",
    "    ## Spot instance\n",
    "    #use_spot_instances   = True,\n",
    "    #max_wait             = 5400,\n",
    "    checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints'\n",
    ")\n",
    "\n",
    "\n",
    "# We need to have ssh key files generated. It is required for ssh passwordless login\n",
    "def gen_ssh_key():\n",
    "    if not os.path.isfile('scripts/id_rsa'):\n",
    "        print('Generating SSH key files for passwordless remote on cluster')\n",
    "        os.system(\"ssh-keygen -f scripts/id_rsa -t rsa -N ''\")\n",
    "\n",
    "\n",
    "gen_ssh_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3beb0138-d0d9-423a-8469-7901730dab16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:59:22 Starting - Starting the training job...\n",
      "2024-06-08 17:59:22 Pending - Training job waiting for capacity...\n",
      "2024-06-08 18:00:16 Pending - Preparing the instances for training...\n",
      "2024-06-08 18:00:51 Downloading - Downloading input data.........\n",
      "2024-06-08 18:01:56 Downloading - Downloading the training image.........\n",
      "2024-06-08 18:03:43 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:09,221 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:09,243 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:09,254 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:09,257 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:10,406 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:09,358 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:09,379 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:09,391 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:09,393 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:10,460 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.41.1 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for transformers==4.41.1 from https://files.pythonhosted.org/packages/79/e1/dcba5ba74392015ceeababf3455138f5875202e66e3316d7ca223bdb7b1c/transformers-4.41.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.41.1 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers==4.41.1 from https://files.pythonhosted.org/packages/79/e1/dcba5ba74392015ceeababf3455138f5875202e66e3316d7ca223bdb7b1c/transformers-4.41.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 5.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.19.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets==2.19.1 from https://files.pythonhosted.org/packages/9f/8a/3922b6d4a8fb40db454abd5d66b28215b047563564f044de693643d5d07f/datasets-2.19.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.30.1 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for accelerate==0.30.1 from https://files.pythonhosted.org/packages/e9/bb/1edd2c836071e91d2bd331b9542bbd592e23d1474645b9c6fd56232caace/accelerate-0.30.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.2 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for evaluate==0.4.2 from https://files.pythonhosted.org/packages/c2/d6/ff9baefc8fc679dcd9eb21b29da3ef10c81aa36be630a7ae78e4611588e1/evaluate-0.4.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.14.2 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.2.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 84.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 4.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting datasets==2.19.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for datasets==2.19.1 from https://files.pythonhosted.org/packages/9f/8a/3922b6d4a8fb40db454abd5d66b28215b047563564f044de693643d5d07f/datasets-2.19.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mCollecting accelerate==0.30.1 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for accelerate==0.30.1 from https://files.pythonhosted.org/packages/e9/bb/1edd2c836071e91d2bd331b9542bbd592e23d1474645b9c6fd56232caace/accelerate-0.30.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mCollecting evaluate==0.4.2 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for evaluate==0.4.2 from https://files.pythonhosted.org/packages/c2/d6/ff9baefc8fc679dcd9eb21b29da3ef10c81aa36be630a7ae78e4611588e1/evaluate-0.4.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting deepspeed==0.14.2 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading deepspeed-0.14.2.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 70.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting flash-attn>=2.0.0 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 90.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting flash-attn>=2.0.0 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 80.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nltk from https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[35mCollecting rouge-score (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[35mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting nltk (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nltk from https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting py7zr (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for py7zr from https://files.pythonhosted.org/packages/e7/38/783a9591807e1e5fd9cd92ce6e0fe49a4ba5a84fed0f552599881017faa6/py7zr-0.21.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading py7zr-0.21.0-py3-none-any.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[35mCollecting bitsandbytes (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/2f/a4/d8c8c1f69ceb3afdc285d62c65bec8d46900d70e81c9a8b24883001e23f8/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting peft (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for peft from https://files.pythonhosted.org/packages/19/99/c5e0292a6d2a62e95c3dfe674ce9e8f8a9fe5d4835d3c9bb9b3e016f02ae/peft-0.11.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting py7zr (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for py7zr from https://files.pythonhosted.org/packages/e7/38/783a9591807e1e5fd9cd92ce6e0fe49a4ba5a84fed0f552599881017faa6/py7zr-0.21.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading py7zr-0.21.0-py3-none-any.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/2f/a4/d8c8c1f69ceb3afdc285d62c65bec8d46900d70e81c9a8b24883001e23f8/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/19/99/c5e0292a6d2a62e95c3dfe674ce9e8f8a9fe5d4835d3c9bb9b3e016f02ae/peft-0.11.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.3 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.41.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/66/e8/bbbad5c7b49c68def42830f96c606e693bfa935a886740a363f04cb84e44/huggingface_hub-0.23.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[35mDownloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: safetensors>=0.3.3 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (0.4.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.41.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for huggingface-hub<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/66/e8/bbbad5c7b49c68def42830f96c606e693bfa935a886740a363f04cb84e44/huggingface_hub-0.23.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers<0.20,>=0.19 (from transformers==4.41.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/0f/cb/8fc733c8f251bac1e5c4ae52458c353b3faa98f41d734c226cad3783da03/tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers==4.41.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/0f/cb/8fc733c8f251bac1e5c4ae52458c353b3faa98f41d734c226cad3783da03/tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.41.1->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.30.1->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.30.1->-r requirements.txt (line 3)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 5)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 5)) (1.10.11)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed==0.14.2->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pynvml from https://files.pythonhosted.org/packages/5b/9c/adb8070059caaa15d5a572b66bccd95900d8c1b9fa54d6ecea6ae97448d1/pynvml-11.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.9/site-packages (from flash-attn>=2.0.0->-r requirements.txt (line 6)) (0.6.1)\u001b[0m\n",
      "\u001b[34mCollecting absl-py (from rouge-score->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for absl-py from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge-score->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 9)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 9)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting texttable (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for texttable from https://files.pythonhosted.org/packages/24/99/4772b8e00a136f3e01236de33b0efda31ee7077203ba5967fcc76da94d65/texttable-1.7.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycryptodomex>=3.16.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pycryptodomex>=3.16.0 from https://files.pythonhosted.org/packages/20/7a/3162173af8597f0399b45c6aaa4939ccae908476fdf1b3a3cc30631fc9fb/pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyzstd>=0.15.9 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pyzstd>=0.15.9 from https://files.pythonhosted.org/packages/48/7a/72ef80b8cb07ace1a8894e2f93666794ed1440f821ec96d67d7ce05cb906/pyzstd-0.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pyzstd-0.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (14.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (2.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (2023.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets==2.19.1->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.30.1->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.30.1->-r requirements.txt (line 3)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 5)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.14.2->-r requirements.txt (line 5)) (1.10.11)\u001b[0m\n",
      "\u001b[35mCollecting pynvml (from deepspeed==0.14.2->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for pynvml from https://files.pythonhosted.org/packages/5b/9c/adb8070059caaa15d5a572b66bccd95900d8c1b9fa54d6ecea6ae97448d1/pynvml-11.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: einops in /opt/conda/lib/python3.9/site-packages (from flash-attn>=2.0.0->-r requirements.txt (line 6)) (0.6.1)\u001b[0m\n",
      "\u001b[35mCollecting absl-py (from rouge-score->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for absl-py from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge-score->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 9)) (8.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 9)) (1.3.0)\u001b[0m\n",
      "\u001b[35mCollecting texttable (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for texttable from https://files.pythonhosted.org/packages/24/99/4772b8e00a136f3e01236de33b0efda31ee7077203ba5967fcc76da94d65/texttable-1.7.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting pycryptodomex>=3.16.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for pycryptodomex>=3.16.0 from https://files.pythonhosted.org/packages/20/7a/3162173af8597f0399b45c6aaa4939ccae908476fdf1b3a3cc30631fc9fb/pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\u001b[0m\n",
      "\u001b[35mCollecting pyzstd>=0.15.9 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for pyzstd>=0.15.9 from https://files.pythonhosted.org/packages/48/7a/72ef80b8cb07ace1a8894e2f93666794ed1440f821ec96d67d7ce05cb906/pyzstd-0.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading pyzstd-0.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\u001b[0m\n",
      "\u001b[35mCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for pyppmd<1.2.0,>=1.1.0 from https://files.pythonhosted.org/packages/df/92/f0a7a6e372c4bd659b5528ff179676522aa72bd8c7a071e757a490ff988e/pyppmd-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading pyppmd-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pyppmd<1.2.0,>=1.1.0 from https://files.pythonhosted.org/packages/df/92/f0a7a6e372c4bd659b5528ff179676522aa72bd8c7a071e757a490ff988e/pyppmd-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pyppmd-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting pybcj<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pybcj<1.1.0,>=1.0.0 from https://files.pythonhosted.org/packages/ac/b2/26fa2cba6bc488380515929757cafbdbf01f30184a1aa11ef7ee35bb21a2/pybcj-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pybcj-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting multivolumefile>=0.2.3 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for multivolumefile>=0.2.3 from https://files.pythonhosted.org/packages/22/31/ec5f46fd4c83185b806aa9c736e228cb780f13990a9cf4da0beb70025fcc/multivolumefile-0.2.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting inflate64<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for inflate64<1.1.0,>=1.0.0 from https://files.pythonhosted.org/packages/c0/c0/417e5183543445818930b3fe181d718e519d26a227b5b77871d8f0c8502d/inflate64-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading inflate64-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting brotli>=1.1.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for brotli>=1.1.0 from https://files.pythonhosted.org/packages/e2/e6/4a730f6e5b5d538e92d09bc51bf69119914f29a222f9e1d65ae4abb27a4e/Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (5.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.1->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.41.1->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.41.1->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.41.1->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.41.1->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.19.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.19.1->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.19.1->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[35mCollecting pybcj<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for pybcj<1.1.0,>=1.0.0 from https://files.pythonhosted.org/packages/ac/b2/26fa2cba6bc488380515929757cafbdbf01f30184a1aa11ef7ee35bb21a2/pybcj-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading pybcj-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting multivolumefile>=0.2.3 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for multivolumefile>=0.2.3 from https://files.pythonhosted.org/packages/22/31/ec5f46fd4c83185b806aa9c736e228cb780f13990a9cf4da0beb70025fcc/multivolumefile-0.2.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting inflate64<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for inflate64<1.1.0,>=1.0.0 from https://files.pythonhosted.org/packages/c0/c0/417e5183543445818930b3fe181d718e519d26a227b5b77871d8f0c8502d/inflate64-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading inflate64-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting brotli>=1.1.0 (from py7zr->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for brotli>=1.1.0 from https://files.pythonhosted.org/packages/e2/e6/4a730f6e5b5d538e92d09bc51bf69119914f29a222f9e1d65ae4abb27a4e/Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (5.5 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.1->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.41.1->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.41.1->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.41.1->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.41.1->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.19.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.19.1->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.19.1->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 96.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 64.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 56.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 24.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 56.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading py7zr-0.21.0-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.6/67.6 kB 20.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 81.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 19.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 50.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 24.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 79.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading py7zr-0.21.0-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.6/67.6 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 52.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 93.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 59.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading inflate64-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.0/93.0 kB 26.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mDownloading pybcj-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.5/49.5 kB 15.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 90.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyppmd-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.8/138.8 kB 37.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyzstd-0.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 413.8/413.8 kB 63.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 98.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 19.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 15.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, flash-attn, rouge-score\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading peft-0.11.1-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 40.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 92.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 62.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading inflate64-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.0/93.0 kB 17.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[35mDownloading pybcj-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.5/49.5 kB 14.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 93.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading pyppmd-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.8/138.8 kB 32.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading pyzstd-0.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 413.8/413.8 kB 60.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 95.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 33.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 12.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: deepspeed, flash-attn, rouge-score\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.2-py3-none-any.whl size=1432250 sha256=478634414f6a56e482a40dd5dca567e6635668a0c456b84cad6d9f4575864410\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/82/53/7d/ca92cb2e9122e1125e714b88b90a926e90036743fa714d52b5\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for deepspeed: filename=deepspeed-0.14.2-py3-none-any.whl size=1432244 sha256=445cb2939f0c2316d24310819e895271744be2d3fc8c3a9317c29617dcae6f83\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/82/53/7d/ca92cb2e9122e1125e714b88b90a926e90036743fa714d52b5\u001b[0m\n",
      "\u001b[35mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp39-cp39-linux_x86_64.whl size=122236719 sha256=cb041a7da64d2ec380ad7c7698fb0afc69ebe728c43bf32873abc0efd58f2f0a\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/4c/0d/c3/1683e92b75450a694f0fcfd7efcbf9a20ceb2ef71c9a89e77c\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=403883f21ea3debd45ac5e99fdff262862f787c2f0eebf96b8f03712c89c3a06\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed flash-attn rouge-score\u001b[0m\n",
      "\u001b[35mCreated wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp39-cp39-linux_x86_64.whl size=122236719 sha256=cb041a7da64d2ec380ad7c7698fb0afc69ebe728c43bf32873abc0efd58f2f0a\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/4c/0d/c3/1683e92b75450a694f0fcfd7efcbf9a20ceb2ef71c9a89e77c\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=403883f21ea3debd45ac5e99fdff262862f787c2f0eebf96b8f03712c89c3a06\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[35mSuccessfully built deepspeed flash-attn rouge-score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, brotli, pyzstd, pyppmd, pynvml, pycryptodomex, pybcj, nltk, multivolumefile, inflate64, absl-py, rouge-score, py7zr, huggingface-hub, flash-attn, deepspeed, bitsandbytes, tokenizers, accelerate, transformers, datasets, peft, evaluate\u001b[0m\n",
      "\u001b[35mInstalling collected packages: texttable, brotli, pyzstd, pyppmd, pynvml, pycryptodomex, pybcj, nltk, multivolumefile, inflate64, absl-py, rouge-score, py7zr, huggingface-hub, flash-attn, deepspeed, bitsandbytes, tokenizers, accelerate, transformers, datasets, peft, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.22.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.22.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[35mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[35mFound existing installation: huggingface-hub 0.22.2\u001b[0m\n",
      "\u001b[35mUninstalling huggingface-hub-0.22.2:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled huggingface-hub-0.22.2\u001b[0m\n",
      "\u001b[35mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[35mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[35mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+4c3ff1a:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[35mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[35mFound existing installation: deepspeed 0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[35mUninstalling deepspeed-0.6.1+4c3ff1a:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled deepspeed-0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[35mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[35mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[35mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.21.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.21.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.21.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[35mFound existing installation: accelerate 0.21.0\u001b[0m\n",
      "\u001b[35mUninstalling accelerate-0.21.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled accelerate-0.21.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[35mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[35mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: evaluate\u001b[0m\n",
      "\u001b[34mFound existing installation: evaluate 0.4.1\u001b[0m\n",
      "\u001b[34mUninstalling evaluate-0.4.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled evaluate-0.4.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-2.1.0 accelerate-0.30.1 bitsandbytes-0.43.1 brotli-1.1.0 datasets-2.19.1 deepspeed-0.14.2 evaluate-0.4.2 flash-attn-2.5.9.post1 huggingface-hub-0.23.3 inflate64-1.0.0 multivolumefile-0.2.3 nltk-3.8.1 peft-0.11.1 py7zr-0.21.0 pybcj-1.0.2 pycryptodomex-3.20.0 pynvml-11.5.0 pyppmd-1.1.0 pyzstd-0.16.0 rouge-score-0.1.2 texttable-1.7.0 tokenizers-0.19.1 transformers-4.41.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[35mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[35mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: evaluate\u001b[0m\n",
      "\u001b[35mFound existing installation: evaluate 0.4.1\u001b[0m\n",
      "\u001b[35mUninstalling evaluate-0.4.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled evaluate-0.4.1\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-2.1.0 accelerate-0.30.1 bitsandbytes-0.43.1 brotli-1.1.0 datasets-2.19.1 deepspeed-0.14.2 evaluate-0.4.2 flash-attn-2.5.9.post1 huggingface-hub-0.23.3 inflate64-1.0.0 multivolumefile-0.2.3 nltk-3.8.1 peft-0.11.1 py7zr-0.21.0 pybcj-1.0.2 pycryptodomex-3.20.0 pynvml-11.5.0 pyppmd-1.1.0 pyzstd-0.16.0 rouge-score-0.1.2 texttable-1.7.0 tokenizers-0.19.1 transformers-4.41.1\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:49,148 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:49,148 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:49,199 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:49,247 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:49,296 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:49,313 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"deepspeed\": \"./configs/mistral_z3_config_bf16.json\",\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"merge_adapters\": false,\n",
      "        \"model_id\": \"mistralai/Mistral-7B-v0.1\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"output_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"per_device_eval_batch_size\": 8,\n",
      "        \"per_device_train_batch_size\": 6,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": 3,\n",
      "        \"tf32\": true,\n",
      "        \"training_script\": \"./scripts/run_qlora.py\",\n",
      "        \"use_flash_attn\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-859967598519/mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds_launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds_launcher.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"deepspeed\":\"./configs/mistral_z3_config_bf16.json\",\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"merge_adapters\":false,\"model_id\":\"mistralai/Mistral-7B-v0.1\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/checkpoints\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":6,\"save_strategy\":\"epoch\",\"save_total_limit\":3,\"tf32\":true,\"training_script\":\"./scripts/run_qlora.py\",\"use_flash_attn\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ds_launcher.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ds_launcher\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-859967598519/mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"deepspeed\":\"./configs/mistral_z3_config_bf16.json\",\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"merge_adapters\":false,\"model_id\":\"mistralai/Mistral-7B-v0.1\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/checkpoints\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":6,\"save_strategy\":\"epoch\",\"save_total_limit\":3,\"tf32\":true,\"training_script\":\"./scripts/run_qlora.py\",\"use_flash_attn\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-859967598519/mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175/source/sourcedir.tar.gz\",\"module_name\":\"ds_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds_launcher.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--deepspeed\",\"./configs/mistral_z3_config_bf16.json\",\"--gradient_checkpointing\",\"True\",\"--hf_token\",\"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\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--merge_adapters\",\"False\",\"--model_id\",\"mistralai/Mistral-7B-v0.1\",\"--num_train_epochs\",\"3\",\"--output_dir\",\"/opt/ml/checkpoints\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"6\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"3\",\"--tf32\",\"True\",\"--training_script\",\"./scripts/run_qlora.py\",\"--use_flash_attn\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DEEPSPEED=./configs/mistral_z3_config_bf16.json\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=false\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=mistralai/Mistral-7B-v0.1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=6\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=3\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_SCRIPT=./scripts/run_qlora.py\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 ds_launcher.py --bf16 True --dataset_path /opt/ml/input/data/training --deepspeed ./configs/mistral_z3_config_bf16.json --gradient_checkpointing True --hf_token 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 --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --merge_adapters False --model_id mistralai/Mistral-7B-v0.1 --num_train_epochs 3 --output_dir /opt/ml/checkpoints --per_device_eval_batch_size 8 --per_device_train_batch_size 6 --save_strategy epoch --save_total_limit 3 --tf32 True --training_script ./scripts/run_qlora.py --use_flash_attn True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2024-06-08 18:04:49,342 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mnum_gpus = 1, num_nodes = 2, current_host = algo-1, rank = 0\u001b[0m\n",
      "\u001b[34mDeepspeed hostfile content:\u001b[0m\n",
      "\u001b[34malgo-1 slots=1\u001b[0m\n",
      "\u001b[34malgo-2 slots=1\u001b[0m\n",
      "\u001b[34mLD_LIBRARY_PATH=/opt/conda/lib/python3.9/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib\u001b[0m\n",
      "\u001b[34mPATH=/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u001b[0m\n",
      "\u001b[34mGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\u001b[0m\n",
      "\u001b[34mGet:2 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:49,301 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:49,301 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:49,352 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:49,398 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:49,446 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:49,464 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"deepspeed\": \"./configs/mistral_z3_config_bf16.json\",\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"merge_adapters\": false,\n",
      "        \"model_id\": \"mistralai/Mistral-7B-v0.1\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"output_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"per_device_eval_batch_size\": 8,\n",
      "        \"per_device_train_batch_size\": 6,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": 3,\n",
      "        \"tf32\": true,\n",
      "        \"training_script\": \"./scripts/run_qlora.py\",\n",
      "        \"use_flash_attn\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-859967598519/mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds_launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds_launcher.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"deepspeed\":\"./configs/mistral_z3_config_bf16.json\",\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"merge_adapters\":false,\"model_id\":\"mistralai/Mistral-7B-v0.1\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/checkpoints\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":6,\"save_strategy\":\"epoch\",\"save_total_limit\":3,\"tf32\":true,\"training_script\":\"./scripts/run_qlora.py\",\"use_flash_attn\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=ds_launcher.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=ds_launcher\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-859967598519/mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"deepspeed\":\"./configs/mistral_z3_config_bf16.json\",\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"merge_adapters\":false,\"model_id\":\"mistralai/Mistral-7B-v0.1\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/checkpoints\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":6,\"save_strategy\":\"epoch\",\"save_total_limit\":3,\"tf32\":true,\"training_script\":\"./scripts/run_qlora.py\",\"use_flash_attn\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-859967598519/mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175/source/sourcedir.tar.gz\",\"module_name\":\"ds_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds_launcher.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--deepspeed\",\"./configs/mistral_z3_config_bf16.json\",\"--gradient_checkpointing\",\"True\",\"--hf_token\",\"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\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--merge_adapters\",\"False\",\"--model_id\",\"mistralai/Mistral-7B-v0.1\",\"--num_train_epochs\",\"3\",\"--output_dir\",\"/opt/ml/checkpoints\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"6\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"3\",\"--tf32\",\"True\",\"--training_script\",\"./scripts/run_qlora.py\",\"--use_flash_attn\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[35mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_DEEPSPEED=./configs/mistral_z3_config_bf16.json\u001b[0m\n",
      "\u001b[35mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[35mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[35mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[35mSM_HP_LR_SCHEDULER_TYPE=constant\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[35mSM_HP_MERGE_ADAPTERS=false\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_ID=mistralai/Mistral-7B-v0.1\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[35mSM_HP_OUTPUT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=6\u001b[0m\n",
      "\u001b[35mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[35mSM_HP_SAVE_TOTAL_LIMIT=3\u001b[0m\n",
      "\u001b[35mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[35mSM_HP_TRAINING_SCRIPT=./scripts/run_qlora.py\u001b[0m\n",
      "\u001b[35mSM_HP_USE_FLASH_ATTN=true\u001b[0m\n",
      "\u001b[35mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 ds_launcher.py --bf16 True --dataset_path /opt/ml/input/data/training --deepspeed ./configs/mistral_z3_config_bf16.json --gradient_checkpointing True --hf_token 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 --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --merge_adapters False --model_id mistralai/Mistral-7B-v0.1 --num_train_epochs 3 --output_dir /opt/ml/checkpoints --per_device_eval_batch_size 8 --per_device_train_batch_size 6 --save_strategy epoch --save_total_limit 3 --tf32 True --training_script ./scripts/run_qlora.py --use_flash_attn True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[35m2024-06-08 18:04:49,492 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mnum_gpus = 1, num_nodes = 2, current_host = algo-2, rank = 1\u001b[0m\n",
      "\u001b[35mDeepspeed hostfile content:\u001b[0m\n",
      "\u001b[35malgo-1 slots=1\u001b[0m\n",
      "\u001b[35malgo-2 slots=1\u001b[0m\n",
      "\u001b[35mLD_LIBRARY_PATH=/opt/conda/lib/python3.9/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib\u001b[0m\n",
      "\u001b[35mPATH=/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u001b[0m\n",
      "\u001b[35mGet:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\u001b[0m\n",
      "\u001b[35mGet:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\u001b[0m\n",
      "\u001b[35mGet:3 http://archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\u001b[0m\n",
      "\u001b[35mGet:4 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\u001b[0m\n",
      "\u001b[35mGet:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\u001b[0m\n",
      "\u001b[35mGet:6 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\u001b[0m\n",
      "\u001b[35mGet:7 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\u001b[0m\n",
      "\u001b[35mGet:8 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\u001b[0m\n",
      "\u001b[35mGet:9 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1509 kB]\u001b[0m\n",
      "\u001b[35mGet:10 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3776 kB]\u001b[0m\n",
      "\u001b[34mGet:3 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3626 kB]\u001b[0m\n",
      "\u001b[34mGet:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\u001b[0m\n",
      "\u001b[34mGet:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\u001b[0m\n",
      "\u001b[34mGet:6 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1211 kB]\u001b[0m\n",
      "\u001b[34mGet:7 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\u001b[0m\n",
      "\u001b[34mGet:8 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.8 kB]\u001b[0m\n",
      "\u001b[34mGet:9 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3680 kB]\u001b[0m\n",
      "\u001b[35mGet:11 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.5 kB]\u001b[0m\n",
      "\u001b[35mGet:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4156 kB]\u001b[0m\n",
      "\u001b[35mGet:13 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\u001b[0m\n",
      "\u001b[35mGet:14 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\u001b[0m\n",
      "\u001b[35mGet:15 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3626 kB]\u001b[0m\n",
      "\u001b[34mGet:10 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\u001b[0m\n",
      "\u001b[34mGet:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\u001b[0m\n",
      "\u001b[34mGet:12 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\u001b[0m\n",
      "\u001b[34mGet:13 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1509 kB]\u001b[0m\n",
      "\u001b[34mGet:14 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.5 kB]\u001b[0m\n",
      "\u001b[34mGet:15 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3776 kB]\u001b[0m\n",
      "\u001b[34mGet:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4156 kB]\u001b[0m\n",
      "\u001b[35mGet:16 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.8 kB]\u001b[0m\n",
      "\u001b[35mGet:17 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3680 kB]\u001b[0m\n",
      "\u001b[34mGet:17 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\u001b[0m\n",
      "\u001b[34mGet:18 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\u001b[0m\n",
      "\u001b[34mFetched 31.6 MB in 8s (4053 kB/s)\u001b[0m\n",
      "\u001b[34mReading package lists...\u001b[0m\n",
      "\u001b[34mReading package lists...\u001b[0m\n",
      "\u001b[34mBuilding dependency tree...\u001b[0m\n",
      "\u001b[34mReading state information...\u001b[0m\n",
      "\u001b[34mThe following additional packages will be installed:\u001b[0m\n",
      "\u001b[34mgenders libgenders0\u001b[0m\n",
      "\u001b[34mSuggested packages:\n",
      "  rdist\u001b[0m\n",
      "\u001b[34mThe following NEW packages will be installed:\u001b[0m\n",
      "\u001b[34mgenders libaio-dev libaio1 libgenders0 pdsh\u001b[0m\n",
      "\u001b[34m0 upgraded, 5 newly installed, 0 to remove and 26 not upgraded.\u001b[0m\n",
      "\u001b[34mNeed to get 188 kB of archives.\u001b[0m\n",
      "\u001b[34mAfter this operation, 620 kB of additional disk space will be used.\u001b[0m\n",
      "\u001b[34mGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgenders0 amd64 1.22-1build2 [29.2 kB]\u001b[0m\n",
      "\u001b[34mGet:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 genders amd64 1.22-1build2 [29.7 kB]\u001b[0m\n",
      "\u001b[34mGet:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libaio1 amd64 0.3.112-5 [7184 B]\u001b[0m\n",
      "\u001b[34mGet:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libaio-dev amd64 0.3.112-5 [13.7 kB]\u001b[0m\n",
      "\u001b[34mGet:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 pdsh amd64 2.31-3build2 [108 kB]\u001b[0m\n",
      "\u001b[34mdebconf: delaying package configuration, since apt-utils is not installed\u001b[0m\n",
      "\u001b[34mFetched 188 kB in 0s (1586 kB/s)\u001b[0m\n",
      "\u001b[34mSelecting previously unselected package libgenders0:amd64.#015\u001b[0m\n",
      "\u001b[34m(Reading database ...\u001b[0m\n",
      "\u001b[34m(Reading database ... 5%#015(Reading database ... 10%#015(Reading database ... 15%#015(Reading database ... 20%#015(Reading database ... 25%#015(Reading database ... 30%#015(Reading database ... 35%#015(Reading database ... 40%#015(Reading database ... 45%#015(Reading database ... 50%#015(Reading database ... 55%\u001b[0m\n",
      "\u001b[34m(Reading database ... 60%\u001b[0m\n",
      "\u001b[34m(Reading database ... 65%\u001b[0m\n",
      "\u001b[34m(Reading database ... 70%\u001b[0m\n",
      "\u001b[34m(Reading database ... 75%\u001b[0m\n",
      "\u001b[34m(Reading database ... 80%\u001b[0m\n",
      "\u001b[34m(Reading database ... 85%\u001b[0m\n",
      "\u001b[34m(Reading database ... 90%\u001b[0m\n",
      "\u001b[34m(Reading database ... 95%\u001b[0m\n",
      "\u001b[34m(Reading database ... 100%#015(Reading database ... 45348 files and directories currently installed.)\u001b[0m\n",
      "\u001b[34mPreparing to unpack .../libgenders0_1.22-1build2_amd64.deb ...\u001b[0m\n",
      "\u001b[34mUnpacking libgenders0:amd64 (1.22-1build2) ...\u001b[0m\n",
      "\u001b[34mSelecting previously unselected package genders.\u001b[0m\n",
      "\u001b[34mPreparing to unpack .../genders_1.22-1build2_amd64.deb ...\u001b[0m\n",
      "\u001b[34mUnpacking genders (1.22-1build2) ...\u001b[0m\n",
      "\u001b[34mSelecting previously unselected package libaio1:amd64.\u001b[0m\n",
      "\u001b[34mPreparing to unpack .../libaio1_0.3.112-5_amd64.deb ...\u001b[0m\n",
      "\u001b[34mUnpacking libaio1:amd64 (0.3.112-5) ...\u001b[0m\n",
      "\u001b[34mSelecting previously unselected package libaio-dev:amd64.\u001b[0m\n",
      "\u001b[34mPreparing to unpack .../libaio-dev_0.3.112-5_amd64.deb ...\u001b[0m\n",
      "\u001b[34mUnpacking libaio-dev:amd64 (0.3.112-5) ...\u001b[0m\n",
      "\u001b[34mSelecting previously unselected package pdsh.\u001b[0m\n",
      "\u001b[34mPreparing to unpack .../pdsh_2.31-3build2_amd64.deb ...\u001b[0m\n",
      "\u001b[34mUnpacking pdsh (2.31-3build2) ...\u001b[0m\n",
      "\u001b[34mSetting up libgenders0:amd64 (1.22-1build2) ...\u001b[0m\n",
      "\u001b[34mSetting up genders (1.22-1build2) ...\u001b[0m\n",
      "\u001b[34mSetting up libaio1:amd64 (0.3.112-5) ...\u001b[0m\n",
      "\u001b[34mSetting up pdsh (2.31-3build2) ...\u001b[0m\n",
      "\u001b[34mSetting up libaio-dev:amd64 (0.3.112-5) ...\u001b[0m\n",
      "\u001b[34mProcessing triggers for libc-bin (2.31-0ubuntu9.14) ...\u001b[0m\n",
      "\u001b[34malgo-1 is waiting for all worker nodes to become ready\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.171.179' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mcat: /tmp/jj_ready: No such file or directory\u001b[0m\n",
      "\u001b[34mNode algo-2 is not ready\u001b[0m\n",
      "\u001b[35mGet:18 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1211 kB]\u001b[0m\n",
      "\u001b[35mFetched 31.6 MB in 17s (1856 kB/s)\u001b[0m\n",
      "\u001b[35mReading package lists...\u001b[0m\n",
      "\u001b[35mReading package lists...\u001b[0m\n",
      "\u001b[35mBuilding dependency tree...\u001b[0m\n",
      "\u001b[35mReading state information...\u001b[0m\n",
      "\u001b[35mThe following additional packages will be installed:\u001b[0m\n",
      "\u001b[35mgenders libgenders0\u001b[0m\n",
      "\u001b[35mSuggested packages:\n",
      "  rdist\u001b[0m\n",
      "\u001b[35mThe following NEW packages will be installed:\u001b[0m\n",
      "\u001b[35mgenders libaio-dev libaio1 libgenders0 pdsh\u001b[0m\n",
      "\u001b[35m0 upgraded, 5 newly installed, 0 to remove and 26 not upgraded.\u001b[0m\n",
      "\u001b[35mNeed to get 188 kB of archives.\u001b[0m\n",
      "\u001b[35mAfter this operation, 620 kB of additional disk space will be used.\u001b[0m\n",
      "\u001b[35mGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgenders0 amd64 1.22-1build2 [29.2 kB]\u001b[0m\n",
      "\u001b[35mGet:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 genders amd64 1.22-1build2 [29.7 kB]\u001b[0m\n",
      "\u001b[35mGet:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libaio1 amd64 0.3.112-5 [7184 B]\u001b[0m\n",
      "\u001b[35mGet:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libaio-dev amd64 0.3.112-5 [13.7 kB]\u001b[0m\n",
      "\u001b[35mGet:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 pdsh amd64 2.31-3build2 [108 kB]\u001b[0m\n",
      "\u001b[35mdebconf: delaying package configuration, since apt-utils is not installed\u001b[0m\n",
      "\u001b[35mFetched 188 kB in 0s (2118 kB/s)\u001b[0m\n",
      "\u001b[35mSelecting previously unselected package libgenders0:amd64.#015\u001b[0m\n",
      "\u001b[35m(Reading database ...\u001b[0m\n",
      "\u001b[35m(Reading database ... 5%#015(Reading database ... 10%#015(Reading database ... 15%#015(Reading database ... 20%#015(Reading database ... 25%#015(Reading database ... 30%#015(Reading database ... 35%#015(Reading database ... 40%#015(Reading database ... 45%#015(Reading database ... 50%#015(Reading database ... 55%\u001b[0m\n",
      "\u001b[35m(Reading database ... 60%\u001b[0m\n",
      "\u001b[35m(Reading database ... 65%\u001b[0m\n",
      "\u001b[35m(Reading database ... 70%\u001b[0m\n",
      "\u001b[35m(Reading database ... 75%\u001b[0m\n",
      "\u001b[35m(Reading database ... 80%\u001b[0m\n",
      "\u001b[35m(Reading database ... 85%\u001b[0m\n",
      "\u001b[35m(Reading database ... 90%\u001b[0m\n",
      "\u001b[35m(Reading database ... 95%\u001b[0m\n",
      "\u001b[35m(Reading database ... 100%#015(Reading database ... 45348 files and directories currently installed.)\u001b[0m\n",
      "\u001b[35mPreparing to unpack .../libgenders0_1.22-1build2_amd64.deb ...\u001b[0m\n",
      "\u001b[35mUnpacking libgenders0:amd64 (1.22-1build2) ...\u001b[0m\n",
      "\u001b[35mSelecting previously unselected package genders.\u001b[0m\n",
      "\u001b[35mPreparing to unpack .../genders_1.22-1build2_amd64.deb ...\u001b[0m\n",
      "\u001b[35mUnpacking genders (1.22-1build2) ...\u001b[0m\n",
      "\u001b[35mSelecting previously unselected package libaio1:amd64.\u001b[0m\n",
      "\u001b[35mPreparing to unpack .../libaio1_0.3.112-5_amd64.deb ...\u001b[0m\n",
      "\u001b[35mUnpacking libaio1:amd64 (0.3.112-5) ...\u001b[0m\n",
      "\u001b[35mSelecting previously unselected package libaio-dev:amd64.\u001b[0m\n",
      "\u001b[35mPreparing to unpack .../libaio-dev_0.3.112-5_amd64.deb ...\u001b[0m\n",
      "\u001b[35mUnpacking libaio-dev:amd64 (0.3.112-5) ...\u001b[0m\n",
      "\u001b[35mSelecting previously unselected package pdsh.\u001b[0m\n",
      "\u001b[35mPreparing to unpack .../pdsh_2.31-3build2_amd64.deb ...\u001b[0m\n",
      "\u001b[35mUnpacking pdsh (2.31-3build2) ...\u001b[0m\n",
      "\u001b[35mSetting up libgenders0:amd64 (1.22-1build2) ...\u001b[0m\n",
      "\u001b[35mSetting up genders (1.22-1build2) ...\u001b[0m\n",
      "\u001b[35mSetting up libaio1:amd64 (0.3.112-5) ...\u001b[0m\n",
      "\u001b[35mSetting up pdsh (2.31-3build2) ...\u001b[0m\n",
      "\u001b[35mSetting up libaio-dev:amd64 (0.3.112-5) ...\u001b[0m\n",
      "\u001b[35mProcessing triggers for libc-bin (2.31-0ubuntu9.14) ...\u001b[0m\n",
      "\u001b[35mNode algo-2 is waiting for node 0 to distribute work\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-1 is waiting for all worker nodes to become ready\u001b[0m\n",
      "\u001b[34mAll worker nodes are ready\u001b[0m\n",
      "\u001b[34mLaunch deepspeed on first node command = deepspeed --hostfile=./deepspeed_hostfile ./scripts/run_qlora.py --bf16 True --dataset_path /opt/ml/input/data/training --deepspeed ./configs/mistral_z3_config_bf16.json --gradient_checkpointing True --hf_token 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 --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --merge_adapters False --model_id mistralai/Mistral-7B-v0.1 --num_train_epochs 3 --output_dir /opt/ml/checkpoints --per_device_eval_batch_size 8 --per_device_train_batch_size 6 --save_strategy epoch --save_total_limit 3 --tf32 True --use_flash_attn True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m[2024-06-08 18:05:11,962] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m[2024-06-08 18:05:14,408] [INFO] [runner.py:463:main] Using IP address of 10.0.190.108 for node algo-1\u001b[0m\n",
      "\u001b[34m[2024-06-08 18:05:14,409] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env\u001b[0m\n",
      "\u001b[34m[2024-06-08 18:05:14,409] [INFO] [multinode_runner.py:81:get_cmd] Running on the following workers: algo-1,algo-2\u001b[0m\n",
      "\u001b[34m[2024-06-08 18:05:14,410] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w algo-1,algo-2 export PYTHONIOENCODING=UTF-8; export NCCL_DEBUG=WARN; export NCCL_SOCKET_IFNAME=eth0; export PYTHONUNBUFFERED=1; export PYTHONDONTWRITEBYTECODE=1; export NCCL_IB_DISABLE=1; export PYTHONPATH=/opt/ml/code:/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages; export NCCL_VERSION=2.14.3; export LD_LIBRARY_PATH=/opt/conda/lib/python3.9/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib; export PATH=/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin;  cd /opt/ml/code; /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJhbGdvLTEiOiBbMF0sICJhbGdvLTIiOiBbMF19 --node_rank=%n --master_addr=10.0.190.108 --master_port=29500 ./scripts/run_qlora.py --bf16 True --dataset_path /opt/ml/input/data/training --deepspeed ./configs/mistral_z3_config_bf16.json --gradient_checkpointing True --hf_token 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 --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --merge_adapters False --model_id mistralai/Mistral-7B-v0.1 --num_train_epochs 3 --output_dir /opt/ml/checkpoints --per_device_eval_batch_size 8 --per_device_train_batch_size 6 --save_strategy epoch --save_total_limit 3 --tf32 True --use_flash_attn True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:15,952] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:16,024] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34malgo-1: #033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34malgo-2: #033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34malgo-1: #033[93m [WARNING] #033[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34malgo-2: #033[93m [WARNING] #033[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,017] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,017] [INFO] [launch.py:139:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,017] [INFO] [launch.py:139:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,017] [INFO] [launch.py:139:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,017] [INFO] [launch.py:146:main] WORLD INFO DICT: {'algo-1': [0], 'algo-2': [0]}\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,017] [INFO] [launch.py:152:main] nnodes=2, num_local_procs=1, node_rank=0\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,017] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'algo-1': [0], 'algo-2': [1]})\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,017] [INFO] [launch.py:164:main] dist_world_size=2\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,017] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:18,026] [INFO] [launch.py:256:main] process 793 spawned with command: ['/opt/conda/bin/python3.9', '-u', './scripts/run_qlora.py', '--local_rank=0', '--bf16', 'True', '--dataset_path', '/opt/ml/input/data/training', '--deepspeed', './configs/mistral_z3_config_bf16.json', '--gradient_checkpointing', 'True', '--hf_token', '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', '--learning_rate', '0.0002', '--logging_steps', '10', '--lr_scheduler_type', 'constant', '--max_grad_norm', '0.3', '--merge_adapters', 'False', '--model_id', 'mistralai/Mistral-7B-v0.1', '--num_train_epochs', '3', '--output_dir', '/opt/ml/checkpoints', '--per_device_eval_batch_size', '8', '--per_device_train_batch_size', '6', '--save_strategy', 'epoch', '--save_total_limit', '3', '--tf32', 'True', '--use_flash_attn', 'True', '--warmup_ratio', '0.03']\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,176] [INFO] [launch.py:139:main] 1 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,176] [INFO] [launch.py:139:main] 1 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,176] [INFO] [launch.py:139:main] 1 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,176] [INFO] [launch.py:139:main] 1 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,176] [INFO] [launch.py:146:main] WORLD INFO DICT: {'algo-1': [0], 'algo-2': [0]}\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,176] [INFO] [launch.py:152:main] nnodes=2, num_local_procs=1, node_rank=1\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,176] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'algo-1': [0], 'algo-2': [1]})\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,176] [INFO] [launch.py:164:main] dist_world_size=2\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,176] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:18,188] [INFO] [launch.py:256:main] process 735 spawned with command: ['/opt/conda/bin/python3.9', '-u', './scripts/run_qlora.py', '--local_rank=0', '--bf16', 'True', '--dataset_path', '/opt/ml/input/data/training', '--deepspeed', './configs/mistral_z3_config_bf16.json', '--gradient_checkpointing', 'True', '--hf_token', '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', '--learning_rate', '0.0002', '--logging_steps', '10', '--lr_scheduler_type', 'constant', '--max_grad_norm', '0.3', '--merge_adapters', 'False', '--model_id', 'mistralai/Mistral-7B-v0.1', '--num_train_epochs', '3', '--output_dir', '/opt/ml/checkpoints', '--per_device_eval_batch_size', '8', '--per_device_train_batch_size', '6', '--save_strategy', 'epoch', '--save_total_limit', '3', '--tf32', 'True', '--use_flash_attn', 'True', '--warmup_ratio', '0.03']\u001b[0m\n",
      "\u001b[34malgo-1: Requirement already satisfied: flash-attn==2.5.9.post1 in /opt/conda/lib/python3.9/site-packages (2.5.9.post1)\u001b[0m\n",
      "\u001b[34malgo-1: Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from flash-attn==2.5.9.post1) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34malgo-1: Requirement already satisfied: einops in /opt/conda/lib/python3.9/site-packages (from flash-attn==2.5.9.post1) (0.6.1)\u001b[0m\n",
      "\u001b[34malgo-1: Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->flash-attn==2.5.9.post1) (4.7.1)\u001b[0m\n",
      "\u001b[34malgo-2: Requirement already satisfied: flash-attn==2.5.9.post1 in /opt/conda/lib/python3.9/site-packages (2.5.9.post1)\u001b[0m\n",
      "\u001b[34malgo-2: Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from flash-attn==2.5.9.post1) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34malgo-2: Requirement already satisfied: einops in /opt/conda/lib/python3.9/site-packages (from flash-attn==2.5.9.post1) (0.6.1)\u001b[0m\n",
      "\u001b[34malgo-2: Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->flash-attn==2.5.9.post1) (4.7.1)\u001b[0m\n",
      "\u001b[34malgo-1: WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34malgo-1: \u001b[0m\n",
      "\u001b[34malgo-1: [notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[34malgo-1: [notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34malgo-2: WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34malgo-2: \u001b[0m\n",
      "\u001b[34malgo-2: [notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[34malgo-2: [notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:24,186] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34malgo-1: #033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:24,419] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34malgo-1: #033[93m [WARNING] #033[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34malgo-2: #033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34malgo-2: #033[93m [WARNING] #033[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:26,578] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:05:26,578] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:05:26,854] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34malgo-1: Logging into the Hugging Face Hub with token hf_TySkJao...\u001b[0m\n",
      "\u001b[34malgo-1: The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34malgo-2: Logging into the Hugging Face Hub with token hf_TySkJao...\u001b[0m\n",
      "\u001b[34malgo-2: The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34malgo-1: Token is valid (permission: read).\u001b[0m\n",
      "\u001b[34malgo-1: Your token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34malgo-1: Login successful\u001b[0m\n",
      "\u001b[34malgo-2: Token is valid (permission: read).\u001b[0m\n",
      "\u001b[34malgo-2: Your token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34malgo-2: Login successful\u001b[0m\n",
      "\u001b[34malgo-2: `low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[34malgo-1: `low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-2: #015Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Downloading shards:  50%|█████     | 1/2 [00:23<00:23, 23.82s/it]#015Downloading shards: 100%|██████████| 2/2 [00:41<00:00, 20.38s/it]#015Downloading shards: 100%|██████████| 2/2 [00:41<00:00, 20.90s/it]\u001b[0m\n",
      "\u001b[34malgo-2: The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-1: #015Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Downloading shards:  50%|█████     | 1/2 [00:27<00:27, 27.01s/it]#015Downloading shards: 100%|██████████| 2/2 [00:44<00:00, 21.57s/it]#015Downloading shards: 100%|██████████| 2/2 [00:44<00:00, 22.39s/it]\u001b[0m\n",
      "\u001b[34malgo-1: The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\u001b[0m\n",
      "\u001b[34malgo-2: #015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.73s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]\u001b[0m\n",
      "\u001b[34malgo-2: Found 7 modules to quantize: ['o_proj', 'up_proj', 'q_proj', 'gate_proj', 'v_proj', 'down_proj', 'k_proj']\u001b[0m\n",
      "\u001b[34malgo-2: trainable params: 167,772,160 || all params: 7,409,504,256 || trainable%: 2.2643\u001b[0m\n",
      "\u001b[34malgo-1: #015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.75s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]\u001b[0m\n",
      "\u001b[34malgo-1: Found 7 modules to quantize: ['o_proj', 'k_proj', 'v_proj', 'q_proj', 'up_proj', 'down_proj', 'gate_proj']\u001b[0m\n",
      "\u001b[34malgo-1: trainable params: 167,772,160 || all params: 7,409,504,256 || trainable%: 2.2643\u001b[0m\n",
      "\u001b[34malgo-1: NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34malgo-1: \u001b[0m\n",
      "\u001b[34malgo-1: ip-10-0-190-108:793:872 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1: \u001b[0m\n",
      "\u001b[34malgo-1: ip-10-0-190-108:793:872 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-2: \u001b[0m\n",
      "\u001b[34malgo-2: ip-10-0-171-179:735:816 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-2: \u001b[0m\n",
      "\u001b[34malgo-2: ip-10-0-171-179:735:816 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1: Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34malgo-1: Creating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34malgo-2: Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34malgo-2: Creating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34malgo-2: Detected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34malgo-2: Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34malgo-2: Building extension module fused_adam...\u001b[0m\n",
      "\u001b[34malgo-2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34malgo-1: Detected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34malgo-1: Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34malgo-1: Building extension module fused_adam...\u001b[0m\n",
      "\u001b[34malgo-1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-1: [1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34malgo-2: [1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34malgo-1: [2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34malgo-2: [2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34malgo-1: [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34malgo-1: Loading extension module fused_adam...\u001b[0m\n",
      "\u001b[34malgo-1: Time to load fused_adam op: 27.471205711364746 seconds\u001b[0m\n",
      "\u001b[34malgo-2: [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34malgo-2: Loading extension module fused_adam...\u001b[0m\n",
      "\u001b[34malgo-2: Time to load fused_adam op: 27.607404470443726 seconds\u001b[0m\n",
      "\u001b[34malgo-1: Parameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:06:56.591 ip-10-0-190-108.ec2.internal:793 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:06:56.591 ip-10-0-171-179.ec2.internal:735 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:06:56.652 ip-10-0-190-108.ec2.internal:793 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:06:56.652 ip-10-0-190-108.ec2.internal:793 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:06:56.653 ip-10-0-190-108.ec2.internal:793 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:06:56.653 ip-10-0-171-179.ec2.internal:735 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:06:56.653 ip-10-0-190-108.ec2.internal:793 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:06:56.653 ip-10-0-171-179.ec2.internal:735 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:06:56.654 ip-10-0-171-179.ec2.internal:735 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:06:56.654 ip-10-0-171-179.ec2.internal:735 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-1: {'loss': 1.5228, 'grad_norm': 0.38407418494782336, 'learning_rate': 0.0002, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-1: #015  0%|          | 0/39 [00:00<?, ?it/s]#015  3%|▎         | 1/39 [00:16<10:44, 16.96s/it]#015  5%|▌         | 2/39 [00:31<09:31, 15.43s/it]#015  8%|▊         | 3/39 [00:45<08:51, 14.77s/it]#015 10%|█         | 4/39 [00:59<08:26, 14.46s/it]#015 13%|█▎        | 5/39 [01:13<08:05, 14.29s/it]#015 15%|█▌        | 6/39 [01:27<07:48, 14.19s/it]#015 18%|█▊        | 7/39 [01:41<07:38, 14.32s/it]#015 21%|██        | 8/39 [01:55<07:20, 14.21s/it]#015 23%|██▎       | 9/39 [02:09<07:04, 14.13s/it]#015 26%|██▌       | 10/39 [02:23<06:48, 14.08s/it]#015                                               #015#015 26%|██▌       | 10/39 [02:23<06:48, 14.08s/it]#015 28%|██▊       | 11/39 [02:37<06:33, 14.05s/it]#015 31%|███       | 12/39 [02:51<06:19, 14.04s/it]#015 33%|███▎      | 13/39 [03:05<06:04, 14.02s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34malgo-2:  stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34malgo-1: /opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\u001b[0m\n",
      "\u001b[34malgo-1:   warnings.warn(\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-1: {'loss': 1.3859, 'grad_norm': 0.11692384501289556, 'learning_rate': 0.0002, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-1: #015 36%|███▌      | 14/39 [03:29<07:04, 16.99s/it]#015 38%|███▊      | 15/39 [03:43<06:26, 16.10s/it]#015 41%|████      | 16/39 [03:57<05:55, 15.46s/it]#015 44%|████▎     | 17/39 [04:11<05:30, 15.02s/it]#015 46%|████▌     | 18/39 [04:25<05:08, 14.70s/it]#015 49%|████▊     | 19/39 [04:39<04:49, 14.48s/it]#015 51%|█████▏    | 20/39 [04:53<04:33, 14.39s/it]#015                                               #015#015 51%|█████▏    | 20/39 [04:53<04:33, 14.39s/it]#015 54%|█████▍    | 21/39 [05:07<04:17, 14.29s/it]#015 56%|█████▋    | 22/39 [05:21<04:01, 14.21s/it]#015 59%|█████▉    | 23/39 [05:35<03:46, 14.13s/it]#015 62%|██████▏   | 24/39 [05:49<03:31, 14.09s/it]#015 64%|██████▍   | 25/39 [06:03<03:16, 14.05s/it]#015 67%|██████▋   | 26/39 [06:17<03:02, 14.02s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34malgo-2:  stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-1: {'loss': 1.2745, 'grad_norm': 0.1600333857133099, 'learning_rate': 0.0002, 'epoch': 2.31}\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34malgo-1: #015 69%|██████▉   | 27/39 [06:41<03:23, 16.92s/it]#015 72%|███████▏  | 28/39 [06:55<02:56, 16.04s/it]#015 74%|███████▍  | 29/39 [07:09<02:34, 15.42s/it]#015 77%|███████▋  | 30/39 [07:23<02:14, 14.99s/it]#015                                               #015#015 77%|███████▋  | 30/39 [07:23<02:14, 14.99s/it]#015 79%|███████▉  | 31/39 [07:37<01:57, 14.68s/it]#015 82%|████████▏ | 32/39 [07:51<01:41, 14.47s/it]#015 85%|████████▍ | 33/39 [08:05<01:25, 14.31s/it]#015 87%|████████▋ | 34/39 [08:19<01:11, 14.21s/it]#015 90%|████████▉ | 35/39 [08:33<00:56, 14.14s/it]#015 92%|█████████▏| 36/39 [08:47<00:42, 14.09s/it]#015 95%|█████████▍| 37/39 [09:01<00:28, 14.05s/it]#015 97%|█████████▋| 38/39 [09:15<00:14, 14.03s/it]#015100%|██████████| 39/39 [09:29<00:00, 14.12s/it] stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34malgo-2:  stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34malgo-1: {'train_runtime': 579.2431, 'train_samples_per_second': 0.772, 'train_steps_per_second': 0.067, 'train_loss': 1.3492715786664913, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34malgo-1: #015                                               #015#015100%|██████████| 39/39 [09:38<00:00, 14.12s/it]#015100%|██████████| 39/39 [09:38<00:00, 14.85s/it]\u001b[0m\n",
      "\u001b[34malgo-2: /opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\u001b[0m\n",
      "\u001b[34malgo-2:   warnings.warn(\u001b[0m\n",
      "\u001b[34malgo-1: [2024-06-08 18:16:38,692] [INFO] [launch.py:351:main] Process 793 exits successfully.\u001b[0m\n",
      "\u001b[34malgo-2: [2024-06-08 18:16:38,852] [INFO] [launch.py:351:main] Process 735 exits successfully.\u001b[0m\n",
      "\u001b[35mcat: /tmp/jj_done: No such file or directory\u001b[0m\n",
      "\u001b[35mNode algo-2 is working, check done again in 30s\u001b[0m\n",
      "\u001b[34mTraining FINISHED\u001b[0m\n",
      "\u001b[34mSet Done flag on algo-2\u001b[0m\n",
      "\u001b[34m2024-06-08 18:16:41,095 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-08 18:16:41,095 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-08 18:16:41,095 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2024-06-08 18:17:10,469 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-06-08 18:17:10,469 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-06-08 18:17:10,470 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-06-08 18:17:30 Uploading - Uploading generated training model\n",
      "2024-06-08 18:17:30 Completed - Training job completed\n",
      "Training seconds: 1998\n",
      "Billable seconds: 1998\n",
      "s3://sagemaker-us-east-1-859967598519/mistral-deepspeed-qlora-mistralai-Mistr-2024-06-08-17-59-21-175/output/model/\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)\n",
    "print(huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8da10671-8dc7-4bbb-b62e-33fca77498b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py39\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2024-05-28-09-09-25-791\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2024-05-28-09-09-26-509\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2024-05-28-09-09-26-509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "## Deploy the trained model \n",
    "##     which is already stored in S3\n",
    "#######################\n",
    "\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.1.0\",\n",
    "  session=sess,\n",
    ")\n",
    " \n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")\n",
    "\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    " \n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(8192), # Max length of the generation (including input text)\n",
    "}\n",
    " \n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")\n",
    "\n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44b3f9b9-8a2f-474f-8d09-2860d7b14569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '\\n\\n# compare neural network and svm\\n\\n- start\\n- 1\\n- 2\\n- 3\\n- 4\\n- 5\\n- 6\\n- 7\\n- 8\\n- 9\\n- 10\\n- 11\\n- 12\\n- 13\\n- 14\\n- 15\\n- 16\\n- 17\\n- 18\\n- 19\\n- 20\\n- 21\\n- 22\\n- 23\\n- 24\\n- 25\\n- 26\\n- 27\\n- 28\\n- 29\\n- 30\\n- 31\\n- 32\\n- 33\\n- 34\\n- 35\\n- 36\\n- 37\\n- 38\\n- 39\\n- 40\\n- 41\\n- 42\\n- 43\\n- 44\\n- 45\\n- 46\\n- 47\\n- 48\\n- 49\\n- 50\\n- 51\\n- 52\\n- 53\\n- 54\\n- 55\\n- 56\\n- 57\\n- 58\\n- 59\\n- 60\\n- 61\\n- 62\\n- 63\\n- 64\\n- 65\\n- 66\\n- 67\\n- 68\\n- 69\\n- 70\\n- 71\\n- 72\\n- 73\\n- 74\\n- 75\\n- 76\\n- 77\\n- 78\\n- 79\\n- 80\\n- 81\\n- 82\\n- 83\\n- 84\\n- 85\\n- 86\\n- 87\\n- 88\\n- 89\\n- 90\\n- 91\\n- 92\\n- 93\\n- 94\\n- 95\\n- 96\\n- 97\\n- 98\\n- 99\\n- 100\\n- 101\\n- 102\\n- 103\\n- 104\\n- 105\\n- 106\\n- 107\\n- 108\\n- 109\\n- 110\\n- 111\\n- 112\\n- 113\\n- 114\\n- 115\\n- 116\\n- 117\\n- 118\\n- 119\\n- 120\\n- 121\\n- 122\\n- 123\\n- 124\\n- 125\\n- 126\\n- 127\\n- 128\\n- 129\\n- 130\\n- 131\\n- 132\\n- 133\\n- 134\\n- 135\\n- 136\\n- 137\\n- 138\\n- 139\\n- 140\\n- 141\\n- 142\\n- 143\\n- 144\\n- 145\\n- 146\\n- 147\\n- 148\\n- 149\\n- 150\\n- 151\\n- 152\\n- 153\\n- 154\\n- 155\\n- 156\\n- 157\\n- 158\\n- 159\\n- 160\\n- 161\\n- 162\\n- 163\\n- 164\\n- 165\\n- 166\\n- 167\\n- 168\\n- 169\\n- 170\\n- 171\\n- 172\\n- 173\\n- 174\\n- 175\\n- 176\\n- 177\\n- 178\\n- 179\\n- 180\\n- 181\\n- 182\\n- 183\\n- 184\\n- 185\\n- 186\\n- '}]\n"
     ]
    }
   ],
   "source": [
    "question = {'inputs': 'compare neural network and svm',\n",
    "           \"parameters\": { \n",
    "                \"max_new_tokens\": 2048,                \n",
    "                \"early_stopping\": True\n",
    "}}\n",
    "answer = llm.predict(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eb10a6e-7a4d-400a-b50d-35bd46132a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-tgi-inference-2024-05-28-09-09-25-791\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-tgi-inference-2024-05-28-09-09-26-509\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-tgi-inference-2024-05-28-09-09-26-509\n"
     ]
    }
   ],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ac136-f3ec-4fc2-a392-31dcbdfac7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
