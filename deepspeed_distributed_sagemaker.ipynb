{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbfc3d-60e3-4c45-a205-11a7859c1bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps --quiet s3fs==2023.12.2\n",
    "!pip install --quiet transformers==4.41.1 peft huggingface_hub hf-transfer\n",
    "!sudo apt install -y pigz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f0097-f6a2-4395-9e86-98eff8c7da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Init the aws client\n",
    "###########################\n",
    "import os\n",
    "import sys\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf6c06-e7b6-480c-a4fd-64d0841d4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import config\n",
    " \n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "my_huggingface_token = config.my_huggingface_token;\n",
    "if my_huggingface_token == 'YOUR_HUGGING_FACE_TOKEN':\n",
    "    sys.exit('You need to provide HuggingFace token in config.py file')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=my_huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4707314-a3e2-4298-8872-1b04de234988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_tar_dir = Path(\"../base_models/\" + model_id)\n",
    "model_tar_dir.mkdir(parents=True, exist_ok = True)\n",
    "s3_model_uri = f\"s3://{sess.default_bucket()}/base_models/{model_id}/model.tar.gz\"\n",
    "if len(list(model_tar_dir.glob(\"*.safetensors\"))) == 0:\n",
    "    print(f\"Save the HuggingFace base model to {model_tar_dir}\")\n",
    "    snapshot_download(\n",
    "        repo_id = model_id,\n",
    "        local_dir = str(model_tar_dir),\n",
    "        token = my_huggingface_token,\n",
    "        ignore_patterns=[\"*.msgpack*\", \"*.h5%\", \"*.bin*\"],\n",
    "    )\n",
    "    assert len(list(model_tar_dir.glob(\"*.safetensors\"))) > 0, \"Model download failed\"\n",
    "\n",
    "    parent_dir=os.getcwd()\n",
    "    # change to model dir\n",
    "    os.chdir(str(model_tar_dir))\n",
    "    # use pigz for faster and parallel compression\n",
    "    print(f\"Compressing the model {model_id}\")\n",
    "    !tar -cf model.tar.gz --use-compress-program=pigz *\n",
    "    # change back to parent dir\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "    print(f\"Uploading the model {model_id} to S3\")\n",
    "    from sagemaker.s3 import S3Uploader\n",
    "    s3_model_uri = S3Uploader.upload(local_path=str(model_tar_dir.joinpath(\"model.tar.gz\")), desired_s3_uri=f\"s3://{sess.default_bucket()}/base_models/{model_id}\")\n",
    "    print(f\"model {model_id} uploaded to: {s3_model_uri}\")\n",
    "else:\n",
    "    print(f'The base model {model_id} seems to be already downloaded')\n",
    "\n",
    "print(f\"Model is stored at {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9ad04-47a4-4dbb-9484-3e205a415210",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Load Dolly dataset\n",
    "###########################\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "dataset = load_dataset('databricks/databricks-dolly-15k', split='train')\n",
    "dataset = dataset.select(range(50))\n",
    "print(f'Dataset Dolly size: {len(dataset)}')\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701f792-535f-4177-a844-2762a9a928a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Function to format the dataset to Mistral format\n",
    "###########################\n",
    "\n",
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550472f-c72f-419d-98a5-0bc6df45632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Format the dataset to Mistral format\n",
    "###########################\n",
    "\n",
    "# add utils method to path for loading dataset\n",
    "sys.path.append(\"scripts\")\n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=2048) # We use 2048 as the maximum length for packing\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples after packing: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f9bd3-d6d1-4da1-b3dd-1646f223d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/mistral/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1339f-f679-46ce-b4bf-5908dc06aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed_parameters = {\n",
    "  \"deepspeed\": \"./configs/mistral_z3_config_bf16.json\", # deepspeed config file\n",
    "  \"training_script\": \"./scripts/run_qlora.py\" # real training script, not entrypoint\n",
    "}\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "training_hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 3,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 6,                 # batch size for training  REQUIRED REVIEW\n",
    "  'per_device_eval_batch_size': 8,                  # REQUIRED REVIEW\n",
    "  #'gradient_accumulation_steps': 8,                 # Number of updates steps to accumulate\n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'bf16': True,                                     # use bfloat16 precision\n",
    "  'tf32': True,                                     # use tf32 precision\n",
    "  'learning_rate': 2e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"constant\",                   # learning rate scheduler\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': False,                          # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'save_strategy': \"epoch\",                         # save strategy for checkpoints\n",
    "  'save_total_limit': 3,\n",
    "  'output_dir': '/opt/ml/checkpoints',              # output directory, where to save assets during training\n",
    "                                                    # could be used for checkpointing. The final trained\n",
    "                                                    # model will always be saved to s3 at the end of training\n",
    "  'hf_token': my_huggingface_token,\n",
    "  'model_data': s3_model_uri,\n",
    "}\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "job_name = f'mistral-deepspeed-qlora-{model_id.replace(\"/\",\"-\").replace(\".\",\"-\")}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'ds_launcher.py',    # train script\n",
    "    source_dir           = '.',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 2,                 # the number of instances used for training\n",
    "    max_run              = 3600,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',           # the python version used in the training job\n",
    "    hyperparameters      =  {\n",
    "        **training_hyperparameters,  # the hyperparameters passed to the training job\n",
    "        **deepspeed_parameters,\n",
    "    },\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "\n",
    "    ## Spot instance\n",
    "    #use_spot_instances   = True,\n",
    "    #max_wait             = 5400,\n",
    "    checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints',\n",
    ")\n",
    "\n",
    "\n",
    "# We need to have ssh key files generated. It is required for ssh passwordless login\n",
    "def gen_ssh_key():\n",
    "    if not os.path.isfile('scripts/id_rsa'):\n",
    "        print('Generating SSH key files for passwordless remote on cluster')\n",
    "        os.system(\"ssh-keygen -f scripts/id_rsa -t rsa -N ''\")\n",
    "\n",
    "\n",
    "gen_ssh_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb0138-d0d9-423a-8469-7901730dab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../aws_config .\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)\n",
    "print(huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"])\n",
    "\n",
    "!rm -rf aws_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da10671-8dc7-4bbb-b62e-33fca77498b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## Deploy the trained model \n",
    "##     which is already stored in S3\n",
    "#######################\n",
    "\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.1.0\",\n",
    "  session=sess,\n",
    ")\n",
    " \n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")\n",
    "\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    " \n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(8192), # Max length of the generation (including input text)\n",
    "}\n",
    " \n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")\n",
    "\n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3f9b9-8a2f-474f-8d09-2860d7b14569",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = {'inputs': 'compare neural network and svm',\n",
    "           \"parameters\": { \n",
    "                \"max_new_tokens\": 2048,                \n",
    "                \"early_stopping\": True\n",
    "}}\n",
    "answer = llm.predict(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb10a6e-7a4d-400a-b50d-35bd46132a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ac136-f3ec-4fc2-a392-31dcbdfac7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
